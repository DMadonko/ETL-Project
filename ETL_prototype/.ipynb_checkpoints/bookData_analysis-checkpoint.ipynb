{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import config\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='data'\n",
    "\n",
    "column_names=['ISBN',\n",
    "             'Name',\n",
    "             'Authors',\n",
    "             'Description',\n",
    "             'Language',\n",
    "             'pagesNumber',\n",
    "             'Publisher',\n",
    "             'PublishYear',\n",
    "             'Rating',\n",
    "             'CountsOfReview']\n",
    "\n",
    "mainData=pd.DataFrame(columns = column_names)\n",
    "\n",
    "# Loop through each file in the data directory and load the files in a dataframe for ETL\n",
    "for file in os.listdir(dataDir):\n",
    "    filePath = '' + dataDir + '/' + os.fsdecode(file)\n",
    "    df = pd.read_csv(filePath)\n",
    "    # workaround for files without 'description' column\n",
    "    if 'Description' not in df.columns:\n",
    "        df['Description']=\"None\"\n",
    "    # initial stage of ETL - filter required columns\n",
    "    df = df[['ISBN',\n",
    "             'Name',\n",
    "             'Authors',\n",
    "             'Description',\n",
    "             'Language',\n",
    "             'pagesNumber',\n",
    "             'Publisher',\n",
    "             'PublishYear',\n",
    "             'Rating',\n",
    "             'CountsOfReview']]\n",
    "    # remove non-english characters from Name and Author\n",
    "    df['Name']=df['Name'].str.replace('[^a-zA-Z0-9!@#$%^&*()-+?/`~\"\\':; ]', '')\n",
    "    #df['Authors']=df['Authors'].str.replace('[^a-zA-Z0-9!@#$%^&*()-+?/`~\"\\':; ]', '')\n",
    "    # drop rows with missing values\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    # append CSV data to main dataframe\n",
    "    mainData = mainData.append(df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distinct languages\n",
    "mainData.Language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter languages & drop duplicates\n",
    "enLanguages=['en-US','eng','en-GB','en-CA']\n",
    "mainData2=mainData[mainData.Language.isin(enLanguages)].copy()\n",
    "# sorting will keep the records with most reviews when duplicates are dropped\n",
    "mainData2.sort_values(by='CountsOfReview',ascending=0,inplace=True)\n",
    "mainData2 = mainData[mainData.Language.isin(enLanguages)].drop_duplicates()\n",
    "mainData2.drop_duplicates(subset='ISBN',inplace=True)\n",
    "mainData2.drop_duplicates(subset='Name',inplace=True)\n",
    "# Convert certain columns to INT\n",
    "mainData2['CountsOfReview']=mainData2['CountsOfReview'].astype(int)\n",
    "mainData2['pagesNumber']=mainData2['pagesNumber'].astype(int)\n",
    "mainData2['PublishYear']=mainData2['PublishYear'].astype(int)\n",
    "mainData2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Statistical overview for numerical columns\n",
    "mainData2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investgate why pagesNumber has minumim value of 0. (Mostly they seem to be Audiobooks) \n",
    "low_page_data= mainData2.loc[(mainData2[\"pagesNumber\"] >= 0) & (mainData2[\"pagesNumber\"] <= 5)]\n",
    "low_page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainData2.sort_values(by='Name',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxData = config.maximum_data\n",
    "dataCut=mainData2.head(maxData).reset_index(drop=True)\n",
    "dataCut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise DF's\n",
    "categoryDF = {\"category_id\":[],\n",
    "             \"category_name\":[]}\n",
    "\n",
    "isbn_categoryDF = {\"isbn_no\":[],\n",
    "                   \"category_id\":[]}\n",
    "\n",
    "authorDF = {\"author_id\":[],\n",
    "            \"author_name\":[]}\n",
    "\n",
    "isbn_authorDF = {\"isbn_no\":[],\n",
    "                 \"author_id\":[]}\n",
    "\n",
    "print_typeDF = {\"print_type_id\":[],\n",
    "                \"print_type\":[]}\n",
    "\n",
    "googlebooks_dataDF= {\"isbn_no\":[],\n",
    "                     \"print_type_id\":[],\n",
    "                     \"retail_price\":[]}\n",
    "# get a list of ISBNs\n",
    "isbn = dataCut['ISBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise ID's\n",
    "category_id = 0\n",
    "author_id = 0\n",
    "print_type_id = 0\n",
    "\n",
    "# initialise counters\n",
    "prc_cntr=0\n",
    "record=0\n",
    "recs_fetched=1\n",
    "set_no = 1\n",
    "\n",
    "# create URL\n",
    "url=f'https://www.googleapis.com/books/v1/volumes?key={config.g_key}&q=isbn:'\n",
    "\n",
    "# record runtime\n",
    "startTime = datetime.datetime.now().strftime('%d/%m/%y %H:%M:%S')\n",
    "\n",
    "# loop through ISBNs and do a googlebooks API call\n",
    "for i in isbn:\n",
    "\n",
    "    # GET the API data\n",
    "    response = requests.get(f\"{url}{i}\").json()\n",
    "    prc_cntr += 1\n",
    "    prcnt=round((prc_cntr/maxData)*100,0)\n",
    "\n",
    "    # if response returns data then process the data\n",
    "    if response['totalItems'] != 0:\n",
    "        \n",
    "        # initialise authors list\n",
    "        authors=[]\n",
    "        \n",
    "        print(f\"RECORD {prc_cntr}: {prcnt}% - Processing ISBN No. {i}\")\n",
    "        \n",
    "        # get author data\n",
    "        try:\n",
    "            authors=response['items'][0]['volumeInfo']['authors']\n",
    "        except (KeyError, IndexError):\n",
    "            authors.append(dataCut.loc[dataCut['ISBN'] == i][\"Authors\"].iloc[0])\n",
    "        \n",
    "        # get print_type data\n",
    "        print_type=response['items'][0]['volumeInfo']['printType']\n",
    "        \n",
    "        # get categories data\n",
    "        try:\n",
    "            categories=response['items'][0]['volumeInfo']['categories']\n",
    "        except (KeyError, IndexError):\n",
    "            categories=[]\n",
    "        \n",
    "        # get list price data\n",
    "        try:\n",
    "            listPrice=response['items'][0]['saleInfo']['listPrice']['amount']\n",
    "        except (KeyError, IndexError):\n",
    "            listPrice=0.00   \n",
    "        \n",
    "        # load categories data in objects\n",
    "        if len(categories) > 0:\n",
    "            for c in categories:\n",
    "                cCaps = c.upper()\n",
    "                if cCaps not in categoryDF['category_name']: \n",
    "                    category_id += 1\n",
    "                    categoryDF['category_id'].append(category_id)\n",
    "                    categoryDF['category_name'].append(cCaps)\n",
    "                    finalCatId = category_id\n",
    "                else: \n",
    "                    finalCatId = categoryDF['category_id'][categoryDF['category_name'].index(cCaps)]\n",
    "\n",
    "                isbn_categoryDF['isbn_no'].append(i)\n",
    "                isbn_categoryDF['category_id'].append(finalCatId)\n",
    "        \n",
    "        # load authors data in objects\n",
    "        for a in authors:\n",
    "            aCaps = a.upper()\n",
    "            if aCaps not in authorDF['author_name']: \n",
    "                author_id += 1\n",
    "                authorDF['author_id'].append(author_id)\n",
    "                authorDF['author_name'].append(aCaps)\n",
    "                finalAuthId = author_id\n",
    "            else: \n",
    "                finalAuthId = authorDF['author_id'][authorDF['author_name'].index(aCaps)]\n",
    "\n",
    "            isbn_authorDF['isbn_no'].append(i)\n",
    "            isbn_authorDF['author_id'].append(finalAuthId)\n",
    "        \n",
    "        # load print type data\n",
    "        if print_type not in print_typeDF['print_type']:\n",
    "            ptCaps = print_type.upper()\n",
    "            print_type_id += 1\n",
    "            print_typeDF['print_type_id'].append(print_type_id)\n",
    "            print_typeDF['print_type'].append(ptCaps)\n",
    "            finalPrintId = print_type_id\n",
    "        else:\n",
    "            finalPrintId = print_typeDF['print_type_id'][print_typeDF['print_type'].index(ptCaps)]\n",
    "        \n",
    "        # load google books data\n",
    "        googlebooks_dataDF['isbn_no'].append(i)\n",
    "        googlebooks_dataDF['print_type_id'].append(finalPrintId)\n",
    "        googlebooks_dataDF['retail_price'].append(listPrice)\n",
    "        \n",
    "    else:\n",
    "        # skip if ISBN is not found\n",
    "        print(f\"RECORD {prc_cntr}: {prcnt}% - ISBN not found. Skipping...\")\n",
    "\n",
    "endTime = datetime.datetime.now().strftime('%d/%m/%y %H:%M:%S')\n",
    "\n",
    "# record start and completion time\n",
    "print(f\"START TIME:     {startTime} \\nCOMPLETION TIME: {endTime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert directory of lists to DataFrames\n",
    "categoryDF=pd.DataFrame(categoryDF)\n",
    "isbn_categoryDF=pd.DataFrame(isbn_categoryDF)\n",
    "authorDF=pd.DataFrame(authorDF)\n",
    "isbn_authorDF=pd.DataFrame(isbn_authorDF)\n",
    "print_typeDF=pd.DataFrame(print_typeDF)\n",
    "googlebooks_dataDF=pd.DataFrame(googlebooks_dataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swobabika's Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn_categoryDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn_authorDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_typeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlebooks_dataDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE KAGGLE DATA AND GOOGLE API DATA TO MAIN DATAFRAME FOR ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Book Category Data to main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge isbn_category and category dataframes to get category names for every ISBN. \n",
    "# Rename 'isbn_no' column to 'ISBN' so that we can merge with kaggle dataframe.\n",
    "isbn_category_merge = pd.merge(isbn_categoryDF,categoryDF, on  = \"category_id\")\n",
    "isbn_category = isbn_category_merge.rename(columns = {\"isbn_no\":\"ISBN\"})\n",
    "isbn_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge above dataframe with main dataframe to add category column to main dataframe.\n",
    "maindf_category_merge = pd.merge(dataCut,isbn_category, on  = \"ISBN\")\n",
    "maindf_category_add1 = maindf_category_merge[['ISBN',\n",
    "             'Name',\n",
    "             'Authors',\n",
    "             'Description',\n",
    "             'Language',\n",
    "             'pagesNumber',\n",
    "             'Publisher',\n",
    "             'PublishYear',\n",
    "             'Rating',\n",
    "             'CountsOfReview',\n",
    "              'category_name']]\n",
    "\n",
    "maindf_category_add1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Google Books Author Data and compare with Author data from Kaggle and Google Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge isbn_authorDF and authorDF dataframes to get author names for every ISBN. \n",
    "# Rename 'isbn_no' column to 'ISBN' so that we can merge with kaggle dataframe.\n",
    "isbn_author_merge = pd.merge(isbn_authorDF,authorDF, on  = \"author_id\")\n",
    "isbn_author = isbn_author_merge.rename(columns = {\"isbn_no\":\"ISBN\",\"author_name\":\"author_GB\"})\n",
    "isbn_author = isbn_author[[\"ISBN\",\"author_GB\"]]\n",
    "isbn_author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge above dataframe with main dataframe to add category column to main dataframe.\n",
    "maindf_author_merge = pd.merge(maindf_category_add1 , isbn_author, on  = \"ISBN\")\n",
    "maindf_authorGB_add2 = maindf_author_merge[[\"ISBN\",\"Name\",\"Authors\",\"author_GB\",\"Description\",\"Language\",\"pagesNumber\",\n",
    "                                           \"Publisher\",\"PublishYear\",\"Rating\",\"CountsOfReview\",\"category_name\"]]\n",
    "maindf_authorGB_add2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Retail Price Data to main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge print_typeDF and googlebooks_dataDF dataframes to get print_type and retail price for every ISBN. \n",
    "# Rename 'isbn_no' column to 'ISBN' so that we can merge with kaggle dataframe.\n",
    "printTyp_price_merge = pd.merge(print_typeDF, googlebooks_dataDF, on  = \"print_type_id\")\n",
    "printTyp_price = printTyp_price_merge.rename(columns = {\"isbn_no\":\"ISBN\", })\n",
    "printTyp_price = printTyp_price[[\"ISBN\",\"print_type\",\"retail_price\"]]\n",
    "printTyp_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge above dataframe with main dataframe to add print_type and retail_price column to main dataframe.\n",
    "mainDF_all = pd.merge(maindf_authorGB_add2 , printTyp_price, on  = \"ISBN\")\n",
    "#maindf_all = maindf_printTyp_price_merge[[\"ISBN\",\"Name\",\"Authors\",\"author_GB\",\"Description\",\"Language\",\"pagesNumber\",\n",
    "                                           #\"Publisher\",\"Rating\",\"CountsOfReview\",\"category_name\"]]\n",
    "mainDF_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Publishers in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 publishers for this book data collection.\n",
    "top_publishers = mainDF_all.groupby('Publisher')['Name'].count().sort_values().tail(10)\n",
    "top_publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 publishers with highest number of published books for this book data collection.\n",
    "\n",
    "bplot=top_publishers.plot(kind=\"barh\",figsize=(16,8),color=\"royalblue\")\n",
    "plt.xlabel(\"No. of Published Books\")\n",
    "plt.title(\"Top 10 Publishers between 2004 to 2010\")\n",
    "for b in bplot.patches:\n",
    "    width = b.get_width()\n",
    "    plt.text(0.1+b.get_width(), b.get_y()+0.5*b.get_height(),\n",
    "             '{:2.0f}'.format(width),\n",
    "             ha='center', va='center')\n",
    "#plt.savefig(\"../01-Project_Documents/01-Presentation_Slides/Price_Output/Apt_Top10_Price.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books Published Each Decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the entire year duration\n",
    "# min_year = mainDF_all['PublishYear'].min()\n",
    "# max_year = mainDF_all['PublishYear'].max()\n",
    "# print(f'The books in this dataframe were published between {min_year} - {max_year}')\n",
    "import numpy as np\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins of 10 years too seggregate the data.\n",
    "#bins=np.linspace(min_year,max_year, num=13)\n",
    "bins=np.linspace(1900 ,2020, num=13)\n",
    "labels = [\"1900-1910\",\"1911-1920\", \"1921-1930\",\"1931-1940\", \"1941-1950\",\"1951-1960\",\"1961-1970\",\"1971-1980\",\"1981-1990\",\n",
    "         \"1991-2000\", \"2001-2010\", \"2011-2020\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A separte dataframe with only PublisherYear data to work with\n",
    "decade_df = mainDF_all[[\"PublishYear\"]]\n",
    "decade_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for decade group category. \n",
    "decade_df[\"Decade_group\"] = pd.cut(decade_df[\"PublishYear\"], bins, labels = labels, include_lowest = True).copy()\n",
    "decade_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of instances(books) of each decade group.\n",
    "decade_df = decade_df.groupby(\"Decade_group\").count()\n",
    "decade_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define x and y axis as lists.\n",
    "x = decade_df.index.tolist()\n",
    "y = decade_df['PublishYear'].values.tolist()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bar graph\n",
    "plt.figure(figsize = (16,8))\n",
    "plt.yscale(\"log\")\n",
    "plt.bar(x, y, width= 0.9, align='center',color='orange', edgecolor = 'red')\n",
    "i = 1.0\n",
    "j = 1.0\n",
    "for i in range(len(x)):\n",
    "    plt.annotate(y[i], (-0.1 + i, y[i] + j))\n",
    "plt.legend(labels = ['Total Number of Books'])\n",
    "plt.title(\"Bar plot representing the trend of Total number of Published Books each Decade\")\n",
    "plt.xlabel('Decade Durations')\n",
    "plt.ylabel('Number of Books')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Pagenumbers and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Pagenumbers and ratings.\n",
    "x_values = mainDF_all['pagesNumber']\n",
    "y_values = mainDF_all['Rating']\n",
    "correlation = st.pearsonr(x_values,y_values)\n",
    "print(f'The correlation between the number of pages in a book and rating is: {round(correlation[0],2)}')\n",
    "plt.scatter(x_values,y_values, color = 'blue')\n",
    "plt.xlabel('No.of Pages in a Book')\n",
    "plt.ylabel('Rating')\n",
    "(slope, intercept, rvalue, pvalue, stderr) = linregress(x_values, y_values)\n",
    "regress_values = x_values * slope + intercept\n",
    "line_eq = \"y = \" + str(round(slope,2)) + \"x + \" + str(round(intercept,2))\n",
    "plt.scatter(x_values,y_values)\n",
    "plt.plot(x_values,regress_values,\"r-\")\n",
    "plt.annotate(line_eq,(200,3.5),fontsize=15,color=\"red\")\n",
    "plt.xlabel('***')\n",
    "plt.ylabel('***')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation: There seems to be no correlation between number of pages of a book and it's rating. Hence, number of pages doent seem to affect the likeability of a book at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Type Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTyp_count = mainDF_all['category_name'].value_counts()\n",
    "bookTyp_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTyp_count.plot.pie(startangle=30,autopct='%1.1f%%',figsize=(12, 12) )\n",
    "plt.title('Distribution of Book Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
