{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import config\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='data'\n",
    "\n",
    "column_names=['ISBN',\n",
    "             'Name',\n",
    "             'Authors',\n",
    "             'Description',\n",
    "             'Language',\n",
    "             'pagesNumber',\n",
    "             'Publisher',\n",
    "             'PublishYear',\n",
    "             'Rating',\n",
    "             'CountsOfReview']\n",
    "\n",
    "mainData=pd.DataFrame(columns = column_names)\n",
    "\n",
    "# Loop through each file in the data directory and load the files in a dataframe for ETL\n",
    "for file in os.listdir(dataDir):\n",
    "    filePath = '' + dataDir + '/' + os.fsdecode(file)\n",
    "    df = pd.read_csv(filePath)\n",
    "    # workaround for files without 'description' column\n",
    "    if 'Description' not in df.columns:\n",
    "        df['Description']=\"None\"\n",
    "    # initial stage of ETL - filter required columns\n",
    "    df = df[['ISBN',\n",
    "             'Name',\n",
    "             'Authors',\n",
    "             'Description',\n",
    "             'Language',\n",
    "             'pagesNumber',\n",
    "             'Publisher',\n",
    "             'PublishYear',\n",
    "             'Rating',\n",
    "             'CountsOfReview']]\n",
    "    # remove non-english characters from Name and Author\n",
    "    df['Name']=df['Name'].str.replace('[^a-zA-Z0-9!@#$%^&*()-+?/`~\"\\':; ]', '')\n",
    "    #df['Authors']=df['Authors'].str.replace('[^a-zA-Z0-9!@#$%^&*()-+?/`~\"\\':; ]', '')\n",
    "    # drop rows with missing values\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    # append CSV data to main dataframe\n",
    "    mainData = mainData.append(df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distinct languages\n",
    "mainData.Language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter languages & drop duplicates\n",
    "enLanguages=['en-US','eng','en-GB','en-CA']\n",
    "mainData2=mainData[mainData.Language.isin(enLanguages)].copy()\n",
    "# sorting will keep the records with most reviews when duplicates are dropped\n",
    "mainData2.sort_values(by='CountsOfReview',ascending=0,inplace=True)\n",
    "mainData2 = mainData[mainData.Language.isin(enLanguages)].drop_duplicates()\n",
    "mainData2.drop_duplicates(subset='ISBN',inplace=True)\n",
    "mainData2.drop_duplicates(subset='Name',inplace=True)\n",
    "# Convert certain columns to INT\n",
    "mainData2['CountsOfReview']=mainData2['CountsOfReview'].astype(int)\n",
    "mainData2['pagesNumber']=mainData2['pagesNumber'].astype(int)\n",
    "mainData2['PublishYear']=mainData2['PublishYear'].astype(int)\n",
    "mainData2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainData2.sort_values(by='Name',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxData = config.maximum_data\n",
    "dataCut=mainData2.head(maxData).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise DF's\n",
    "categoryDF = {\"category_id\":[],\n",
    "             \"category_name\":[]}\n",
    "\n",
    "isbn_categoryDF = {\"isbn_no\":[],\n",
    "                   \"category_id\":[]}\n",
    "\n",
    "authorDF = {\"author_id\":[],\n",
    "            \"author_name\":[]}\n",
    "\n",
    "isbn_authorDF = {\"isbn_no\":[],\n",
    "                 \"author_id\":[]}\n",
    "\n",
    "print_typeDF = {\"print_type_id\":[],\n",
    "                \"print_type\":[]}\n",
    "\n",
    "googlebooks_dataDF= {\"isbn_no\":[],\n",
    "                     \"print_type_id\":[],\n",
    "                     \"retail_price\":[]}\n",
    "# get a list of ISBNs\n",
    "isbn = dataCut['ISBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise ID's\n",
    "category_id = 0\n",
    "author_id = 0\n",
    "print_type_id = 0\n",
    "\n",
    "# initialise counters\n",
    "prc_cntr=0\n",
    "record=0\n",
    "recs_fetched=1\n",
    "set_no = 1\n",
    "\n",
    "# create URL\n",
    "url=f'https://www.googleapis.com/books/v1/volumes?key={config.g_key}&q=isbn:'\n",
    "\n",
    "# record runtime\n",
    "startTime = datetime.datetime.now().strftime('%d/%m/%y %H:%M:%S')\n",
    "\n",
    "# loop through ISBNs and do a googlebooks API call\n",
    "for i in isbn:\n",
    "\n",
    "    # GET the API data\n",
    "    response = requests.get(f\"{url}{i}\").json()\n",
    "    prc_cntr += 1\n",
    "    prcnt=round((prc_cntr/maxData)*100,0)\n",
    "\n",
    "    # if response returns data then process the data\n",
    "    if response['totalItems'] != 0:\n",
    "        \n",
    "        # initialise authors list\n",
    "        authors=[]\n",
    "        \n",
    "        print(f\"RECORD {prc_cntr}: {prcnt}% - Processing ISBN No. {i}\")\n",
    "        \n",
    "        # get author data\n",
    "        try:\n",
    "            authors=response['items'][0]['volumeInfo']['authors']\n",
    "        except (KeyError, IndexError):\n",
    "            authors.append(dataCut.loc[dataCut['ISBN'] == i][\"Authors\"].iloc[0])\n",
    "        \n",
    "        # get print_type data\n",
    "        print_type=response['items'][0]['volumeInfo']['printType']\n",
    "        \n",
    "        # get categories data\n",
    "        try:\n",
    "            categories=response['items'][0]['volumeInfo']['categories']\n",
    "        except (KeyError, IndexError):\n",
    "            categories=[]\n",
    "        \n",
    "        # get list price data\n",
    "        try:\n",
    "            listPrice=response['items'][0]['saleInfo']['listPrice']['amount']\n",
    "        except (KeyError, IndexError):\n",
    "            listPrice=0.00   \n",
    "        \n",
    "        # load categories data in objects\n",
    "        if len(categories) > 0:\n",
    "            for c in categories:\n",
    "                cCaps = c.upper()\n",
    "                if cCaps not in categoryDF['category_name']: \n",
    "                    category_id += 1\n",
    "                    categoryDF['category_id'].append(category_id)\n",
    "                    categoryDF['category_name'].append(cCaps)\n",
    "                    finalCatId = category_id\n",
    "                else: \n",
    "                    finalCatId = categoryDF['category_id'][categoryDF['category_name'].index(cCaps)]\n",
    "\n",
    "                isbn_categoryDF['isbn_no'].append(i)\n",
    "                isbn_categoryDF['category_id'].append(finalCatId)\n",
    "        \n",
    "        # load authors data in objects\n",
    "        for a in authors:\n",
    "            aCaps = a.upper()\n",
    "            if aCaps not in authorDF['author_name']: \n",
    "                author_id += 1\n",
    "                authorDF['author_id'].append(author_id)\n",
    "                authorDF['author_name'].append(aCaps)\n",
    "                finalAuthId = author_id\n",
    "            else: \n",
    "                finalAuthId = authorDF['author_id'][authorDF['author_name'].index(aCaps)]\n",
    "\n",
    "            isbn_authorDF['isbn_no'].append(i)\n",
    "            isbn_authorDF['author_id'].append(finalAuthId)\n",
    "        \n",
    "        # load print type data\n",
    "        if print_type not in print_typeDF['print_type']:\n",
    "            ptCaps = print_type.upper()\n",
    "            print_type_id += 1\n",
    "            print_typeDF['print_type_id'].append(print_type_id)\n",
    "            print_typeDF['print_type'].append(ptCaps)\n",
    "            finalPrintId = print_type_id\n",
    "        else:\n",
    "            finalPrintId = print_typeDF['print_type_id'][print_typeDF['print_type'].index(ptCaps)]\n",
    "        \n",
    "        # load google books data\n",
    "        googlebooks_dataDF['isbn_no'].append(i)\n",
    "        googlebooks_dataDF['print_type_id'].append(finalPrintId)\n",
    "        googlebooks_dataDF['retail_price'].append(listPrice)\n",
    "        \n",
    "    else:\n",
    "        # skip if ISBN is not found\n",
    "        print(f\"RECORD {prc_cntr}: {prcnt}% - ISBN not found. Skipping...\")\n",
    "\n",
    "endTime = datetime.datetime.now().strftime('%d/%m/%y %H:%M:%S')\n",
    "\n",
    "# record start and completion time\n",
    "print(f\"START TIME:     {startTime} \\nCOMPLETION TIME: {endTime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert directory of lists to DataFrames\n",
    "categoryDF=pd.DataFrame(categoryDF)\n",
    "isbn_categoryDF=pd.DataFrame(isbn_categoryDF)\n",
    "authorDF=pd.DataFrame(authorDF)\n",
    "isbn_authorDF=pd.DataFrame(isbn_authorDF)\n",
    "print_typeDF=pd.DataFrame(print_typeDF)\n",
    "googlebooks_dataDF=pd.DataFrame(googlebooks_dataDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
